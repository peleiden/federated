% Template for ICASSP-2010 paper; to be used with:
%          mlspconf.sty  - ICASSP/ICIP LaTeX style file adapted for MLSP, and
%          IEEEbib.bst - IEEE bibliography style file.
% --------------------------------------------------------------------------
\documentclass{article}
\usepackage{amsmath,graphicx,02460}

\usepackage{url}
\allowdisplaybreaks

\toappear{02460 Advanced Machine Learning, DTU Compute, Spring 2022}


\newcommand{\code}[1]{{\texttt{\small#1}}}
\newcommand{\numberthis}{\addtocounter{equation}{1}\tag{\theequation}}
\newcommand{\acomm}[1]{\hspace{2.5cm}\text{#1}}
\newcommand{\low}[1]{\ensuremath{_\textup{#1}}}

\newcommand{\andim}{\textup{ and }}
\newcommand{\raq}{\Rightarrow\quad}
\newcommand{\lraq}{\Leftrightarrow\quad}
\newcommand{\qandq}{\quad\wedge\quad}
\newcommand{\qorq}{\quad\vee\quad}
\newcommand{\diff}[2]{\ensuremath{\frac{\md #1}{\md #2}}}
\newcommand{\md}{\ensuremath{\text{d}}}

\newcommand{\ctp}[1]{\ensuremath{\cdot10^{#1}}}
\newcommand{\reci}{\ensuremath{^{-1}}}
\newcommand{\twopow}{\ensuremath{^{2}}}
\newcommand{\re}[1]{\ensuremath{^{#1}}}

\newcommand{\me}{\ensuremath{\operatorname{e}}}
\newcommand{\eul}[1]{\ensuremath{\me^{#1}}}
\newcommand{\len}[1]{\ensuremath{\left\lvert#1\right\rvert}}
\newcommand{\half}{\ensuremath{\frac{1}{2}}}
\newcommand{\third}{\ensuremath{\frac{1}{3}}}
\newcommand{\fourth}{\ensuremath{\frac{1}{4}}}
\newcommand{\transpose}[1]{\ensuremath{#1^{\textup T}}}

\newcommand{\NN}{\ensuremath{\mathbb N}}
\newcommand{\ZZ}{\ensuremath{\mathbb Z}}
\newcommand{\QQ}{\ensuremath{\mathbb Q}}
\newcommand{\RR}{\ensuremath{\mathbb R}}
\newcommand{\CC}{\ensuremath{\mathbb C}}
\newcommand{\LL}{\ensuremath{\mathbb L}}
\newcommand{\PP}{\ensuremath{\mathbb P}}

\newcommand{\unit}[1]{\ensuremath{\:\text{#1}}}
\newcommand{\pro}{\ensuremath{\unit{\%{}}}}

%Kommandoer til ændring af ligestillingsmargner
\newcommand{\jl}[1]{\multicolumn{1}{l}{#1}}
\newcommand{\jc}[1]{\multicolumn{1}{c}{#1}}
\newcommand{\jr}[1]{\multicolumn{1}{r}{#1}}
\newcommand{\jls}[1]{\multicolumn{1}{l|}{#1}}
\newcommand{\jcs}[1]{\multicolumn{1}{c|}{#1}}
\newcommand{\jrs}[1]{\multicolumn{1}{r|}{#1}}

\title{20 Raspberry Pi's, One Model: Federated Learning Over Real Hardware}
\name{Søren Winkel Holm, Asger Laurits Schultz, Gustav Lang Moesmand}
\address{Technical University of Denmark}
%
%
\begin{document}
%\ninept
%

\maketitle
%
\begin{abstract}
    Federated Learning (FL) is emerging as a essential mechanism for assuring user privacy in large-scale machine learning (ML) \cite{kai2021advances}.
    To capture the characteristics of FL methods, the unique setup of aggregating models from a federation of disjoint devices must be simulated realistically.
    Seeking to investigate the resulting practical issues, we install 20 Raspberry Pi's in an experimental configuration for FL over physical clients.
    We setup experiments with hyperparameters in the FedAvg \cite{mcmahan2017communication} algorithm including the number of local device epochs for which we identify a trade-off between spending time on communication and on local computation.
    Testing robustness both against imbalanced data across devices and the addition of rogue, noisy clients, we highlight the potential of using stronger aggregation schemes than averaging by implementing the FedDF \cite{lin2020ensemble} algorithm.
\end{abstract}
%
\begin{keywords}
    Federated Learning, Deep Learning, Privacy, Computer Vision
\end{keywords}

\section{INTRODUCTION}
\label{sec:intro}
Large-scale surveys have shown that the growing use of Artificial Intelligence (AI) has resulted in a widespread fear about loss of personal privacy \cite{beuc2020consumers, west2018survey}.
As a part of a general push towards safer AI, large tech companies such as Google and Apple have employed FL methods in cases such as Siri, Google Chrome and Gboard \cite{kai2021advances}.

FL covers the ML setup where multiple clients collaborate in learning from local data across clients which is not exchanged and a central server aggregates updates \cite{kai2021advances, mcmahan2017communication}.
When the aggregation corresponds to the average over weights from models each produced by training multiple local epochs on each client, the algorithm is the formative \emph{FedAvg}\cite{mcmahan2017communication}.
Many further aggregation methods exist, including the ensemble distillation algorithm \emph{FedDF} \cite{lin2020ensemble}, and we refer to Kairouz et al. for an overview \cite{kai2021advances}.

Across this rich literature, many benchmarks of FL performance over algorithmic choices exist, but are often performed by simulating the federation on central compute clusters.
In this project, we seek to capture the unique hardware setup of FL use-cases such as smartphones where a number of computationally weak edge devices hold the data.
This is performed by performing local training of a convolutional neural network (CNN) on 20 Raspberry Pi devices over which CIFAR-10 \cite{alex2009learning} is divided, and aggregating these centrally using FedAvg.
The aim of the project is to investigate the impact of FedAvg hyperparameters on convergence time, analyze aggregation robustness against imbalanced and noisy data, and uncover performance bottlenecks and tradeoffs in the physical hardware setting.

\section{METHODS}%
\label{sec:methods}
\subsection{Physical Devices}
\begin{enumerate}
    \item Single server HPC 
    \item 20 physical Raspberry Pi's
    \item Networking setup
\end{enumerate}

\subsection{FL Methods}
\begin{enumerate}
    \item FedAvg and all parameters
    \item FedDF
\end{enumerate}

\subsection{Data Imbalance and Noise}
\begin{enumerate}
    \item Dirichlet
    \item Noise approach
\end{enumerate}

\subsection{Deep Learning Problem}
\begin{enumerate}
    \item Architecture
    \item Optimization
    \item Datasets
\end{enumerate}

\subsection{Evaluation}
\begin{enumerate}
    \item Introduce types of experiments
\end{enumerate}

\section{RESULTS}%
\label{sec:results}

\begin{table}
        \footnotesize
        \begin{center}
                \begin{tabular}{l l l l l l}
                        Local epochs & 1 & 10 & 20 & 40 & 80 \\
                        \hline
                        Test acc. [\%] & $47.1 \pm 1.0$ & $56.6 \pm 0.8$ & $56.3 \pm 0.7$ & $55.9 \pm 0.6$ & $54.1 \pm 0.5$ \\
                \end{tabular}
        \end{center}
        \caption{}
        \label{tab:local_epochs}
\end{table}


\begin{table}
        \small
        \begin{center}
                \begin{tabular}{l l l l l}
                        Class balance $\alpha$ & 0.01 & 1.0 & 100.0 & iid \\
                        \hline
                        Test acc. [\%] & $34.8 \pm 3.4$ & $55.4 \pm 0.9$ & $57.4 \pm 0.4$ & $68.6 \pm 0.5$ \\
                \end{tabular}
        \end{center}
        \caption{}
        \label{tab:alpha}
\end{table}
\begin{table}
        \footnotesize
        \begin{center}
                \begin{tabular}{l l l l l l}
                        Noisy clients & 0 & 10 & 20 & 30 & 40 \\
                        \hline
                        Test acc. [\%] & $55.9 \pm 0.5$ & $53.1 \pm 1.4$ & $45.3 \pm 2.3$ & $26.4 \pm 6.9$ & $10.1 \pm 0.6$ \\
                \end{tabular}
        \end{center}
        \caption{}
        \label{tab:noisy_clients}
\end{table}
\begin{table}
        \small
        \begin{center}
                \begin{tabular}{l l l l l}
                        Clients sampled & 5 & 10 & 20 & 40 \\
                        \hline
                        Test acc. [\%] & $53.7 \pm 1.3$ & $55.0 \pm 0.5$ & $56.5 \pm 0.6$ & $57.4 \pm 0.7$ \\
                \end{tabular}
        \end{center}
        \caption{}
        \label{tab:clients_per_round}
\end{table}

\section{DISCUSSION}%
\label{sec:discussion}

\vfill
\pagebreak

\bibliographystyle{IEEEbib}
\bibliography{references}

\end{document}
