% !TeX spellcheck = en_GB
% Template for ICASSP-2010 paper; to be used with:
%          mlspconf.sty  - ICASSP/ICIP LaTeX style file adapted for MLSP, and
%          IEEEbib.bst - IEEE bibliography style file.
% --------------------------------------------------------------------------
\documentclass{article}
\usepackage{amsmath,graphicx,02460}

\usepackage{url}
\allowdisplaybreaks

\toappear{02460 Advanced Machine Learning, DTU Compute, Spring 2022}


\newcommand{\code}[1]{{\texttt{\small#1}}}
\newcommand{\numberthis}{\addtocounter{equation}{1}\tag{\theequation}}
\newcommand{\acomm}[1]{\hspace{2.5cm}\text{#1}}
\newcommand{\low}[1]{\ensuremath{_\textup{#1}}}

\newcommand{\andim}{\textup{ and }}
\newcommand{\raq}{\Rightarrow\quad}
\newcommand{\lraq}{\Leftrightarrow\quad}
\newcommand{\qandq}{\quad\wedge\quad}
\newcommand{\qorq}{\quad\vee\quad}
\newcommand{\diff}[2]{\ensuremath{\frac{\md #1}{\md #2}}}
\newcommand{\md}{\ensuremath{\text{d}}}

\newcommand{\ctp}[1]{\ensuremath{\cdot10^{#1}}}
\newcommand{\reci}{\ensuremath{^{-1}}}
\newcommand{\twopow}{\ensuremath{^{2}}}
\newcommand{\re}[1]{\ensuremath{^{#1}}}

\newcommand{\me}{\ensuremath{\operatorname{e}}}
\newcommand{\eul}[1]{\ensuremath{\me^{#1}}}
\newcommand{\len}[1]{\ensuremath{\left\lvert#1\right\rvert}}
\newcommand{\half}{\ensuremath{\frac{1}{2}}}
\newcommand{\third}{\ensuremath{\frac{1}{3}}}
\newcommand{\fourth}{\ensuremath{\frac{1}{4}}}
\newcommand{\transpose}[1]{\ensuremath{#1^{\textup T}}}

\newcommand{\NN}{\ensuremath{\mathbb N}}
\newcommand{\ZZ}{\ensuremath{\mathbb Z}}
\newcommand{\QQ}{\ensuremath{\mathbb Q}}
\newcommand{\RR}{\ensuremath{\mathbb R}}
\newcommand{\CC}{\ensuremath{\mathbb C}}
\newcommand{\LL}{\ensuremath{\mathbb L}}
\newcommand{\PP}{\ensuremath{\mathbb P}}

\newcommand{\unit}[1]{\ensuremath{\:\text{#1}}}
\newcommand{\pro}{\ensuremath{\unit{\%{}}}}

%Kommandoer til ændring af ligestillingsmargner
\newcommand{\jl}[1]{\multicolumn{1}{l}{#1}}
\newcommand{\jc}[1]{\multicolumn{1}{c}{#1}}
\newcommand{\jr}[1]{\multicolumn{1}{r}{#1}}
\newcommand{\jls}[1]{\multicolumn{1}{l|}{#1}}
\newcommand{\jcs}[1]{\multicolumn{1}{c|}{#1}}
\newcommand{\jrs}[1]{\multicolumn{1}{r|}{#1}}

\title{20 Raspberry Pi's, One Model: Federated Learning On Real Hardware}
\name{Søren Winkel Holm, Asger Laurits Schultz, Gustav Lang Moesmand}
\address{Technical University of Denmark}
%
%
\begin{document}
%\ninept
%

\maketitle
%
\begin{abstract}
    Federated Learning (FL) is emerging as a essential mechanism for assuring user privacy in large-scale machine learning (ML) \cite{kai2021advances}.
    Important use-cases of this learning paradigm run on a federation of real-world user devices, but often in the literature, FL is simulated in an artificial computing cluster environment \cite{kai2021advances,mcmahan2017communication,lin2020ensemble}.
    Seeking to capture the unique FL problems and trade-offs when running on physical hardware, we install a network of 20 Raspberry Pi's acting as user devices.
    Using this experimental setup, we perform an empirical study of the influence of key hyperparameters the FedAvg \cite{mcmahan2017communication} algorithm.
    For the number of local device epochs, the physical timings allow us to identify a trade-off between spending time on communication and on local computation.
    Testing robustness both against imbalanced data across devices and the addition of rogue, noisy clients, we highlight the potential of using stronger aggregation schemes than averaging by implementing the FedDF \cite{lin2020ensemble} algorithm.
\end{abstract}
%
\begin{keywords}
    Federated Learning, Deep Learning, Privacy, Computer Vision
\end{keywords}

\section{INTRODUCTION}
\label{sec:intro}
Large-scale surveys have shown that the growing use of Artificial Intelligence (AI) has resulted in a widespread fear about loss of personal privacy \cite{beuc2020consumers, west2018survey}.
As a part of a general push towards safer AI, large tech companies such as Google and Apple have employed FL methods in cases such as Siri, Google Chrome and Gboard \cite{kai2021advances}.

The term FL covers the distributed ML setup where multiple clients collaborate in learning from local datasets which are not exchanged and where a central server aggregates updates \cite{kai2021advances, mcmahan2017communication}.
When the aggregation is performed by iteratively averaging weights from models each produced by training for a number local epochs on each client, the resulting FL algorithm is the formative \emph{FedAvg}\cite{mcmahan2017communication}.
To gain faster convergence and better data imbalance robustness, further aggregation methods have been developed, including the ensemble distillation algorithm \emph{FedDF} \cite{lin2020ensemble}, and we refer to Kairouz et al. for an overview of the field\cite{kai2021advances}.

Across this rich literature, many benchmarks of FL performance over algorithmic choices exist, but are often performed by simulating the federation on central compute clusters.
In this project, we seek to capture the unique hardware setup of FL use-cases such as smartphones where a number of computationally weak edge devices hold the data.
This is performed by performing local training of a convolutional neural network (CNN) on 20 Raspberry Pi devices over which CIFAR-10 \cite{alex2009learning} is divided, and aggregating these centrally using FedAvg.
The aim of the project is to investigate the impact of FedAvg hyperparameters on convergence time, analyze aggregation robustness against imbalanced and noisy data, and uncover performance bottlenecks and tradeoffs in the physical hardware setting.

\section{METHODS}%
\label{sec:methods}

\subsection{FL Methods}
\begin{enumerate}
    \item FedAvg and all parameters
    \item FedDF
\end{enumerate}

\subsection{Physical Devices}
The project setup is divided into two pieces: A central high-performance cluster (HPC) and 20 Raspberry Pi's.
Crucially, the Pi's are located on a network seperate from the HPC to get more realistic communication overhead.
The HPC is responsible for aggregating the local models trained by the Pi's and evaluating the resulting global model.
The code was designed to not require physical devices, so experiments where running time was not of interest could be run on the A100 for faster training time and a reduced power bill.

Every Pi runs a Flask server that receives a model every communication round and returning the trained local model along with telemetry data such as memory usage, which is an important consideration when running on such resource-limited devices.
The Flask server also had a route for sending commands to allow for primitive over-the-air-update functionality.
The Pi's were made accessible to the HPC using port-forwarding.

Each Pi is connected to a switch, which in turn is connected via cable to the router.
If the switches are turned off, the Pi's connect to the router via Wi-Fi, which allows testing the impact of communication overhead in two cases: under relatively fast ethernet and relatively slow Wi-Fi.
For reference, the network used had a bandwidth of 100 Mbit/s both up and down, all of which would be utilized on ethernet, but only about 40\pro\ utilized on Wi-Fi.

\subsection{Data Imbalance and Noise}
\begin{enumerate}
    \item Dirichlet
    \item Noise approach
\end{enumerate}

\subsection{Deep Learning Problem}
\begin{enumerate}
    \item Architecture
    \item Optimization
    \item Datasets
\end{enumerate}

\subsection{Evaluation}
\begin{enumerate}
    \item Introduce types of experiments
\end{enumerate}

\section{RESULTS}%
\label{sec:results}

\begin{table}
        \footnotesize
        \begin{center}
                \begin{tabular}{l l l l l l}
                        Local epochs & 1 & 10 & 20 & 40 & 80 \\
                        \hline
                        Test acc. [\%] & $47.1 \pm 1.0$ & $56.6 \pm 0.8$ & $56.3 \pm 0.7$ & $55.9 \pm 0.6$ & $54.1 \pm 0.5$ \\
                \end{tabular}
        \end{center}
        \caption{}
        \label{tab:local_epochs}
\end{table}


\begin{table}
        \small
        \begin{center}
                \begin{tabular}{l l l l l}
                        Class balance $\alpha$ & 0.01 & 1.0 & 100.0 & iid \\
                        \hline
                        Test acc. [\%] & $34.8 \pm 3.4$ & $55.4 \pm 0.9$ & $57.4 \pm 0.4$ & $68.6 \pm 0.5$ \\
                \end{tabular}
        \end{center}
        \caption{}
        \label{tab:alpha}
\end{table}
\begin{table}
        \footnotesize
        \begin{center}
                \begin{tabular}{l l l l l l}
                        Noisy clients & 0 & 10 & 20 & 30 & 40 \\
                        \hline
                        Test acc. [\%] & $55.9 \pm 0.5$ & $53.1 \pm 1.4$ & $45.3 \pm 2.3$ & $26.4 \pm 6.9$ & $10.1 \pm 0.6$ \\
                \end{tabular}
        \end{center}
        \caption{}
        \label{tab:noisy_clients}
\end{table}
\begin{table}
        \small
        \begin{center}
                \begin{tabular}{l l l l l}
                        Clients sampled & 5 & 10 & 20 & 40 \\
                        \hline
                        Test acc. [\%] & $53.7 \pm 1.3$ & $55.0 \pm 0.5$ & $56.5 \pm 0.6$ & $57.4 \pm 0.7$ \\
                \end{tabular}
        \end{center}
        \caption{}
        \label{tab:clients_per_round}
\end{table}

\section{DISCUSSION}%
\label{sec:discussion}

\vfill
\pagebreak

\bibliographystyle{IEEEbib}
\bibliography{references}

\end{document}
