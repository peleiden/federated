% !TeX spellcheck = en_GB
% Template for ICASSP-2010 paper; to be used with:
%          mlspconf.sty  - ICASSP/ICIP LaTeX style file adapted for MLSP, and
%          IEEEbib.bst - IEEE bibliography style file.
% --------------------------------------------------------------------------
\documentclass{article}
\usepackage{amsmath,graphicx,02460}
\usepackage{amssymb}
\usepackage{bm}

\usepackage[dvipsnames]{xcolor}

\usepackage{pgf,tikz}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations.markings}

\usepackage{url}
\allowdisplaybreaks

\toappear{02460 Advanced Machine Learning, DTU Compute, Spring 2022}


\newcommand{\code}[1]{{\texttt{\small#1}}}
\newcommand{\numberthis}{\addtocounter{equation}{1}\tag{\theequation}}
\newcommand{\acomm}[1]{\hspace{2.5cm}\text{#1}}
\newcommand{\low}[1]{\ensuremath{_\textup{#1}}}

\newcommand{\andim}{\textup{ and }}
\newcommand{\raq}{\Rightarrow\quad}
\newcommand{\lraq}{\Leftrightarrow\quad}
\newcommand{\qandq}{\quad\wedge\quad}
\newcommand{\qorq}{\quad\vee\quad}
\newcommand{\diff}[2]{\ensuremath{\frac{\md #1}{\md #2}}}
\newcommand{\md}{\ensuremath{\text{d}}}

\newcommand{\ctp}[1]{\ensuremath{\cdot10^{#1}}}
\newcommand{\reci}{\ensuremath{^{-1}}}
\newcommand{\twopow}{\ensuremath{^{2}}}
\newcommand{\re}[1]{\ensuremath{^{#1}}}

\newcommand{\me}{\ensuremath{\operatorname{e}}}
\newcommand{\eul}[1]{\ensuremath{\me^{#1}}}
\newcommand{\len}[1]{\ensuremath{\left\lvert#1\right\rvert}}
\newcommand{\half}{\ensuremath{\frac{1}{2}}}
\newcommand{\third}{\ensuremath{\frac{1}{3}}}
\newcommand{\fourth}{\ensuremath{\frac{1}{4}}}
\newcommand{\transpose}[1]{\ensuremath{#1^{\textup T}}}

\newcommand{\NN}{\ensuremath{\mathbb N}}
\newcommand{\ZZ}{\ensuremath{\mathbb Z}}
\newcommand{\QQ}{\ensuremath{\mathbb Q}}
\newcommand{\RR}{\ensuremath{\mathbb R}}
\newcommand{\CC}{\ensuremath{\mathbb C}}
\newcommand{\LL}{\ensuremath{\mathbb L}}
\newcommand{\PP}{\ensuremath{\mathbb P}}

\newcommand{\unit}[1]{\ensuremath{\:\text{#1}}}
\newcommand{\pro}{\ensuremath{\unit{\%{}}}}

%Kommandoer til ændring af ligestillingsmargner
\newcommand{\jl}[1]{\multicolumn{1}{l}{#1}}
\newcommand{\jc}[1]{\multicolumn{1}{c}{#1}}
\newcommand{\jr}[1]{\multicolumn{1}{r}{#1}}
\newcommand{\jls}[1]{\multicolumn{1}{l|}{#1}}
\newcommand{\jcs}[1]{\multicolumn{1}{c|}{#1}}
\newcommand{\jrs}[1]{\multicolumn{1}{r|}{#1}}

\title{20 Raspberry Pi's, One Model: Federated Learning On Real Hardware}
\name{Søren Winkel Holm, Asger Laurits Schultz, Gustav Lang Moesmand}
\address{Technical University of Denmark}
%
%
\begin{document}
%\ninept
%

\maketitle
%
\begin{abstract}
    Federated Learning (FL) is emerging as a essential mechanism for assuring user privacy in large-scale machine learning (ML) \cite{kai2021advances}.
    Important use-cases of this learning paradigm run on a federation of real-world user devices, but often in the literature, FL is simulated in an artificial computing cluster environment \cite{kai2021advances,mcmahan2017communication,lin2020ensemble}.
    Seeking to capture the unique FL problems and trade-offs when running on physical hardware, we install a network of 20 Raspberry Pi's acting as user devices.
    Using this experimental setup, we perform an empirical study of the influence of key hyperparameters the FedAvg \cite{mcmahan2017communication} algorithm.
    For the number of local device epochs, the physical timings allow us to identify a trade-off between spending time on communication and on local computation.
    Testing robustness both against imbalanced data across devices and the addition of rogue, noisy clients, we highlight the potential of using stronger aggregation schemes than averaging by implementing the FedDF \cite{lin2020ensemble} algorithm.
\end{abstract}
%
\begin{keywords}
    Federated Learning, Deep Learning, Privacy, Computer Vision
\end{keywords}


\section{INTRODUCTION}
\label{sec:intro}
Large-scale surveys have shown that the growing use of Artificial Intelligence (AI) has resulted in a widespread fear about loss of personal privacy \cite{beuc2020consumers, west2018survey}.
As a part of a general push towards safer AI, large tech companies such as Google and Apple have employed FL methods in cases such as Siri, Google Chrome and Gboard \cite{kai2021advances}.

The term FL covers the distributed ML setup where multiple clients collaborate in learning from local datasets which are not exchanged and where a central server aggregates updates \cite{kai2021advances, mcmahan2017communication}.
When the aggregation is performed by iteratively averaging weights from models each produced by training for a number local epochs on each client, the resulting FL algorithm is the formative \emph{FedAvg}\cite{mcmahan2017communication}.
To gain faster convergence and better data imbalance robustness, further aggregation methods have been developed, including the ensemble distillation algorithm \emph{FedDF} \cite{lin2020ensemble}, and we refer to Kairouz et al. for an overview of the field\cite{kai2021advances}.

Across this rich literature, many benchmarks of FL performance over algorithmic choices exist, but are often performed by simulating the federation on central compute clusters.
In this project, we seek to capture the unique hardware setup of FL use-cases such as smartphones where a number of computationally weak edge devices hold the data.
This is performed by performing local training of a convolutional neural network (CNN) on 20 Raspberry Pi devices over which CIFAR-10 \cite{alex2009learning} is divided, and aggregating these centrally using FedAvg.
The aim of the project is to investigate the impact of FedAvg hyperparameters on convergence time, analyze aggregation robustness against imbalanced and noisy data, and uncover performance bottlenecks and tradeoffs in the physical hardware setting.

\section{METHODS}%
\label{sec:methods}

\subsection{FL Methods}
We implemented FL by setting up $K$ clients each with a disjoint partition of the full training dataset.
A global model $\mathcal M_G$ was initialized and maintained on the central server, and for $L$ iterations, dubbed communication rounds, $S \leq K$ clients were sampled, each receiving a copy of $\mathcal M_G$.
On each client, $E$ local epochs of gradient-based minibatch learning of the local dataset were performed before returning the updated model $\mathcal M_k$ to the server.

For FedAvg, the server aggregated the $S$ returned models by averaging over all model weights, yielding a new global model to be sent out for next communication round.

% TODO: FedDF

\begin{figure}[ht!]
    \centering
    \input{imgs/setup.tikz}
    \caption{The federated setup performing updates at communication round $l$ where Raspberry Pi 1 trains a local model as client $k_1$ and Pi 20 trains as client $k_{20}$.}
    \label{fig:setup}
\end{figure}\noindent

\subsection{Physical Devices}
The project setup, shown on Figure \ref{fig:setup}, was divided into two pieces: A central high-performance cluster (HPC) and 20 Raspberry Pi 3B's, each with 1 GB memory and a quad-core 1.2 GHz CPU.
Crucially, the Pi's were located on a network separate from the HPC to simulate more realistic communication overhead.
The HPC server was responsible for aggregating the local models trained by the Pi's and evaluating the resulting global model.
The code was designed to not require physical devices, so experiments without physical timings interest could be run on an NVIDIA A100 for faster training time and a reduced power bill.

Every Pi ran a Flask server that received $\mathcal M_G^{(l)}$ every communication round $l$ and returned the trained local model along with telemetry data such as memory usage, which was an important consideration when running on such resource-limited devices.
The Flask server also had a route for sending commands to allow for primitive over-the-air-update functionality.
The Pi's were made accessible to the HPC using port-forwarding.

This implementation stored the entire training dataset on each physical device, making it possible to simulate more than 20 clients ($K > 20$) as long as no more than 20 clients were sampled each round ($S \leq 20$).

Each Pi was connected to a switch, which in turn was connected via cable to the router.
If the switches were turned off, the Pi's connected to the router via Wi-Fi, which allows for testing the impact of communication overhead in two cases: under relatively fast ethernet and relatively slow Wi-Fi.
For reference, the network used had a bandwidth of 100 Mbit/s both up and down, all of which would be utilized on ethernet, but only about 40\pro\ on Wi-Fi.

\subsection{Deep Learning Problem}
For an example learning problem, we chose the CIFAR-10 computer vision (CV) task of classifying $32\times 32$ images into object classes including bird, cat and airplane \cite{alex2009learning}.
Due to device memory limits, all images were greyscaled.

The training dataset contains 50K images of 10 equally common classes.
For the model $\mathcal M$, we chose a small network with two convolutional layers followed by two linear layers.

Optimization for the $E$ local epochs on each device was performed by using the Adam optimizer \cite{kingma2015adam} with a learning rate of $\eta_1 = 5\cdot 10^{-4}$ and a batch size of 16.
The learning rate was lowered each local epoch to $\eta_{t+1} = 0.995\cdot \eta_{t}$.

\subsection{Data Imbalance and Noise}
\subsubsection{Dirichlet sampling}
The training data was evenly divided into $K$ pieces among all the clients.
A simple way to distribute data is to scramble and divide it into $K$ contiguous chunks, which would make the label distribution within each client uniform in the limit.

However, in practice, such balance in the dataset cannot be assumed.
In order to simulate varying levels of label balance, the Dirichlet distribution, $\operatorname{Dir}(\bm\alpha)$, was used.
Here, $\bm\alpha$ is a 10 long vector with every $\alpha_i$ corresponding to a label.
Furthermore, we let every $\alpha_i=\alpha$ be the same value.
Every sample $\bm\pi\sim\operatorname{Dir}(\bm\alpha)$ will be a probability distribution over labels, the uniformity of which is directly determined by $\alpha$.
For $\alpha\to0$, one label dominates, where as for $\alpha\to\infty$, $\bm\pi$ will be increasingly uniform.
Pleasingly, for $\alpha=1$, every possible $\bm\pi$ is equally likely.

$\bm\pi$ was sampled for every client, making the label distribution Dirichlet for every client.
However, this will cause some labels to be oversampled and some undersampled.
A property of $\bm\pi$ is that reordering it still makes it follow the same distribution.
$\bm\pi$ is thus reordered for each client, until a mostly even label distribution across all devices is achieved, no matter the level of balance within devices.

\subsubsection{Noisy data}
Noise can be added to the data by randomly replacing labels.
This is controlled by two parameters; $N_K$ is the number of noisy clients, and $p$ is the fraction of randomly replaced labels within each noisy client.
For simplicity, $p=1$ is always used.

\subsection{Evaluation}
\begin{enumerate}
    \item Introduce types of experiments
\end{enumerate}

\section{RESULTS}%
\label{sec:results}

\begin{table}[htb!]
    \centering
\resizebox{\linewidth}{!}{
    \begin{tabular}{lllll}
       \hline
        \multicolumn{5}{c}{Varying clients sampled}\\
        5 & 10 & 20 & 40 \\
       \hline
        $53.7 \pm 1.3$ & $55.0 \pm 0.4$ & $56.5 \pm 0.6$ & $57.4 \pm 0.6$ \\
        \multicolumn{5}{c}{Varying class balance ($\alpha$)}\\
        0.01 & 1.0 & 100.0 & iid \\
       \hline
        $34.8 \pm 3.3$ & $55.4 \pm 0.8$ & $57.4 \pm 0.4$ & $68.6 \pm 0.4$ \\
        \multicolumn{5}{c}{Varying noisy clients}\\
        0 & 10 & 20 & 30 & 40 \\
       \hline
        $55.9 \pm 0.5$ & $53.1 \pm 1.2$ & $45.3 \pm 2.1$ & $26.4 \pm 6.1$ & $10.1 \pm 0.5$ \\
        \multicolumn{5}{c}{Varying local epochs}\\
        1 & 10 & 20 & 40 & 80 \\
       \hline
        $47.1 \pm 0.9$ & $56.6 \pm 0.7$ & $56.3 \pm 0.6$ & $55.9 \pm 0.5$ & $54.1 \pm 0.5$ \\
\end{tabular}}
    \caption{
    Test accuracy [\%] of final models using FedAvg over $K=40$ clients when running for $L=20$ local epochs.
    Each run is repeated five times with an approximate 95\%\ confidence interval suggested.
    }
    \label{tab:main}
\end{table}

\section{DISCUSSION}%
\label{sec:discussion}

\vfill
\pagebreak

\bibliographystyle{IEEEbib}
\bibliography{references}

\appendix
\section{Model}
\label{app:model}
The model architecture is described below, listing the sequential operations in the foward pass.
\begin{table}[htb!]
    \centering
    \begin{tabular}{l|p{5cm}}
            Layer type & Hyperparameters\\
            \hline
            2D Convolution & 1 in-channel, 16 out-channels, $3\times 3$ kernel, stride of 1\\
            ReLU activation\\
            2D Convolution & 16 in-channels, 32 out-channels, $3\times 3$ kernel, stride of 1\\
            ReLU activation\\
            2D MaxPooling & $2\times 2$ kernel, stride of 2, no padding, dilation of 1\\
            Dropout & $p=50\%$\\
            Flattening\\
            Linear w. bias & $6,272$ features in, 64 features out\\
            ReLU \\
            Dropout & $p=50\%$\\
            Linear w. bias & 64 feature in, 10 features out
        \end{tabular}
\end{table}\noindent



\end{document}
